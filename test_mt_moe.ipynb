{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    ")\n",
    "from transformer_utils.data_utils_inference import *\n",
    "import copy\n",
    "from typing import Dict\n",
    "import torch\n",
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "from transformer_utils.modeling_moe import *\n",
    "import numpy as np\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForSequenceClassificationMT, BertTokenizer),\n",
    "    # 'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "    'xlm': (XLMConfig, XLMForSequenceClassificationMT, XLMTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaForSequenceClassificationMT, RobertaTokenizer),\n",
    "    # 'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer),\n",
    "    # 'albert': (AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer),\n",
    "    'xlmroberta': (XLMRobertaConfig, XLMRobertaForSequenceClassificationMT, XLMRobertaTokenizerFast),\n",
    "    'xlmrobertafast': (XLMRobertaConfig, XLMRobertaForSequenceClassificationMT, XLMRobertaTokenizerFast),\n",
    "    'robertatextcnn': (RobertaConfig, RobertaTextCNNForSequenceClassificationMT, RobertaTokenizer),\n",
    "    'xlmrobertatextcnn': (XLMRobertaConfig, XLMRobertaTextCNNForSequenceClassificationMT, XLMRobertaTokenizer),\n",
    "\n",
    "    'robertatextlstm': (RobertaConfig, RobertaLSTMForSequenceClassificationMT, RobertaTokenizer),\n",
    "    'xlmrobertalstm': (XLMRobertaConfig, XLMRobertaLSTMForSequenceClassificationMT, XLMRobertaTokenizer),\n",
    "\n",
    "    'robertadpcnn': (RobertaConfig, RobertaDPCNNForSequenceClassificationMT, RobertaTokenizer),\n",
    "    'xlmrobertadpcnn': (XLMRobertaConfig, XLMRobertaDPCNNForSequenceClassificationMT, XLMRobertaTokenizer),\n",
    "    'deberta': (DebertaConfig, DebertaForSequenceClassificationMT,DebertaTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    def __init__(self):\n",
    "        self.add_visual_features = True\n",
    "        self.max_bounding_box_size = 200\n",
    "        self.max_color_size = 100\n",
    "        self.max_FontWeight_size = 100\n",
    "        self.max_FontSize_size = 100\n",
    "        self.equal_size = 2\n",
    "        self.max_part_size = 10\n",
    "        self.output_hidden_states = True\n",
    "        self.num_attention_heads = 12\n",
    "        self.num_hidden_layers = 12\n",
    "        self.intermediate_size = 3072\n",
    "        self.batch_size = 16\n",
    "        self.hidden_size = 768\n",
    "        self.model_type = \"xlmroberta\"\n",
    "        self.drop_hard_label = False\n",
    "        self.test_onnx = False\n",
    "        self.vocab_clip_mapping_file = None\n",
    "        self.output_path = \"result.tsv\"\n",
    "        self.sliding_window_size = -1\n",
    "        self.cache_dir = None\n",
    "        self.no_cuda = False\n",
    "        self.local_rank = -1\n",
    "        self.inference_mt_all_tasks = False\n",
    "        self.abandon_visual_features = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_config(config):\n",
    "    config.add_visual_features = args.add_visual_features\n",
    "    config.max_bounding_box_size = args.max_bounding_box_size\n",
    "    config.max_color_size = args.max_color_size\n",
    "    config.max_FontWeight_size = args.max_FontWeight_size\n",
    "    config.max_FontSize_size = args.max_FontSize_size\n",
    "    config.equal_size = args.equal_size\n",
    "    config.max_part_size = args.max_part_size\n",
    "    config.drop_hard_label = args.drop_hard_label\n",
    "    config.sliding_window_size = args.sliding_window_size\n",
    "    # logger.info(\"PyTorch: setting up devices\")\n",
    "    if args.no_cuda:\n",
    "        device = torch.device(\"cpu\")\n",
    "        n_gpu = 0\n",
    "    elif args.local_rank == -1:\n",
    "        # if n_gpu is > 1 we'll use nn.DataParallel.\n",
    "        # If you only want to use a specific subset of GPUs use `CUDA_VISIBLE_DEVICES=0`\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "        device_ids = list(range(n_gpu))\n",
    "        random.shuffle(device_ids)\n",
    "        device = torch.device(\"cuda:{}\".format(device_ids[0]) if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        # Here, we'll use torch.distributed.\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend=\"nccl\", timeout=datetime.timedelta(0, 50000))\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "    config.device = device\n",
    "    config.n_gpu = n_gpu\n",
    "    config.device_ids = device_ids\n",
    "    config.num_attention_heads = args.num_attention_heads\n",
    "    config.hidden_size = args.hidden_size\n",
    "    config.intermediate_size = args.intermediate_size\n",
    "    config.num_hidden_layers = args.num_hidden_layers\n",
    "    config.output_hidden_states = args.output_hidden_states\n",
    "    config.batch_size = args.batch_size\n",
    "    config.test_onnx = args.test_onnx\n",
    "    config.abandon_visual_features = args.abandon_visual_features\n",
    "    config.inference_mt_all_tasks = args.inference_mt_all_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_conll_file_to_overlap_file(conll_file_path, nodrop_no_label=False):\n",
    "    overlap_conll_file = []\n",
    "    content = []\n",
    "    split_ = [i * 250 for i in range(10)]\n",
    "    firstline = False\n",
    "    number = 0\n",
    "    conll_file = open(conll_file_path, 'r', encoding='utf-8')\n",
    "    for line in conll_file:\n",
    "        line_v = line.split(' ')\n",
    "        if len(line_v) != 23:\n",
    "            number += 1\n",
    "            if number % 1000 == 0:\n",
    "                print(number)\n",
    "            if firstline:\n",
    "                if not content:\n",
    "                    continue\n",
    "                has_label = False\n",
    "                for line_tmp in content:\n",
    "                    if 'O' != line_tmp.split(' ')[1]:\n",
    "                        has_label = True\n",
    "                if has_label:\n",
    "                    for part, start_index in enumerate(split_):\n",
    "                        overlap_conll_file.append(xml_path)\n",
    "                        for tmp_content in content[start_index: start_index + 400]:\n",
    "                            overlap_conll_file.append(tmp_content + ' ' + str(part) + '\\n')\n",
    "                        if start_index + 400 >= len(content):\n",
    "                            break\n",
    "                else:\n",
    "                    pass\n",
    "#                     print(xml_path)\n",
    "            xml_path = line\n",
    "            firstline = True\n",
    "            content = []\n",
    "        else:\n",
    "            content.append(line.replace('\\n', ''))\n",
    "    if content:\n",
    "        if nodrop_no_label:\n",
    "            for part, start_index in enumerate(split_):\n",
    "                    overlap_conll_file.append(xml_path)\n",
    "                    for tmp_content in content[start_index: start_index + 400]:\n",
    "                        overlap_conll_file.append(tmp_content + ' ' + str(part) + '\\n')\n",
    "                    if start_index + 400 >= len(content):\n",
    "                        break\n",
    "        elif has_label:\n",
    "            for part, start_index in enumerate(split_):\n",
    "                    overlap_conll_file.append(xml_path)\n",
    "                    for tmp_content in content[start_index: start_index + 400]:\n",
    "                        overlap_conll_file.append(tmp_content + ' ' + str(part) + '\\n')\n",
    "                    if start_index + 400 >= len(content):\n",
    "                        break\n",
    "        else:\n",
    "            pass\n",
    "    return overlap_conll_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class InputExample_pred_mt:\n",
    "    xml_path: str\n",
    "    words: List[str]\n",
    "\n",
    "\n",
    "def get_eval_dataloader_mt(eval_dataset: Dataset, batch_size=20) -> DataLoader:\n",
    "    data_loader = DataLoader(\n",
    "        dataset=eval_dataset,\n",
    "        sampler=None,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def inference_mt(dataloader, model, task_index, args=None):\n",
    "    # logger.info(\"***** Running %s *****\")\n",
    "    # logger.info(\"  Num examples = %d\", len(dataloader.dataset))\n",
    "    # logger.info(\"  Batch size = %d\", dataloader.batch_size)\n",
    "\n",
    "    preds: np.ndarray = None\n",
    "    label_ids: np.ndarray = None\n",
    "    session = None\n",
    "    # print(f\"Output logits for task {args.task_index}\")\n",
    "    if args.test_onnx:\n",
    "        session = rt.InferenceSession(model, providers=['CPUExecutionProvider'])\n",
    "    else:\n",
    "        if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "            model = torch.nn.DataParallel(model, args.device_ids)\n",
    "        model.to(args.device)\n",
    "        model.eval()\n",
    "    for inputs in tqdm(dataloader, desc='Evaluate'):\n",
    "        has_labels = any(inputs.get(k) is not None for k in [\"labels\", \"masked_lm_labels\"])\n",
    "        for k in list(inputs.keys()):\n",
    "            if k not in ['input_ids', 'attention_mask', 'visual_features', 'labels']:\n",
    "                inputs.pop(k)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(args.device)\n",
    "        if args.test_onnx:\n",
    "            onnx_inputs = {session.get_inputs()[0].name: to_numpy(inputs['input_ids']),\n",
    "                           session.get_inputs()[1].name: to_numpy(inputs['attention_mask']),\n",
    "                           session.get_inputs()[2].name: to_numpy(inputs['visual_features'])}\n",
    "            ort_outs = session.run(['output'], onnx_inputs)\n",
    "            pred = ort_outs[0]\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, task_index=task_index)\n",
    "                if has_labels:  \n",
    "                    step_eval_loss, logits = outputs[:2] \n",
    "                else:\n",
    "                    logits = outputs[0]\n",
    "                pred = logits.detach().cpu().numpy()\n",
    "        if preds is None:\n",
    "            preds = pred\n",
    "        else:\n",
    "            preds = np.append(preds, pred, axis=0)\n",
    "        if inputs.get(\"labels\") is not None:\n",
    "            if label_ids is None:\n",
    "                label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                label_ids = np.append(label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "    return label_ids, preds\n",
    "\n",
    "\n",
    "def get_real_label_mt(predictions: np.ndarray, label_ids: np.ndarray, label_map: Dict, label2id: Dict):\n",
    "    # predictions = softmax(predictions, axis=2)  \n",
    "    preds_task = predictions\n",
    "#     preds_max = np.max(predictions, axis=2)\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    out_label_list, preds_list = [[[] for _ in range(batch_size)] for i in range(2)]\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if label_ids[i, j] != nn.NLLLoss().ignore_index:\n",
    "                out_label_list[i].append(label_map[label_ids[i][j]])\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "    return preds_list, out_label_list   # delete the useless tokens\n",
    "\n",
    "\n",
    "\n",
    "def get_real_result_from_real_preds_add_mt(real_preds, examples, real_label_ids):\n",
    "    xml_dic = {}\n",
    "    real_preds_ori, real_label_ids_ori, examples_ori, label_real_from_example_ori = [[] for i in range(4)]\n",
    "    for index, example in enumerate(examples):\n",
    "        if xml_dic.__contains__(example.xml_path) is False:\n",
    "            xml_dic[example.xml_path] = [index]\n",
    "        else:\n",
    "            xml_dic[example.xml_path].append(index)\n",
    "    \n",
    "    for key in xml_dic:\n",
    "        IsImage = [[0 for i in range(250 * (len(xml_dic[key]) - 1))] for j in range(1)]\n",
    "        label_pred, label_real_from_example, label_real = [['O' for i in range(250 * (len(xml_dic[key]) - 1))] for j in range(3)]\n",
    "        words = ['' for i in range(250 * (len(xml_dic[key]) - 1))]\n",
    "        for part, index in enumerate(xml_dic[key]):\n",
    "            words[part * 250: part * 250 + len(examples[index].words_ori)] = examples[index].words_ori\n",
    "            IsImage[part * 250: part * 250 + len(examples[index].isImage)] = examples[index].isImage\n",
    "            label_real_from_example[part * 250: part * 250 + len(examples[index].labels)] = examples[index].labels\n",
    "            label_real[part * 250: part * 250 + len(real_label_ids[index])] = real_label_ids[index]\n",
    "            tmp = list(real_preds[index])\n",
    "            for index_, lab in enumerate(label_pred[part * 250:]):\n",
    "                if index_ == len(tmp):\n",
    "                    break\n",
    "                if tmp[index_] == \"O\" and lab != \"O\":\n",
    "                    tmp[index_] = lab\n",
    "            label_pred[part * 250: part * 250 + len(tmp)] = tmp\n",
    "        examples_ori.append(InputExample_pred_mt(xml_path=key, words=words))\n",
    "        real_preds_ori.append(label_pred)\n",
    "        label_real_from_example_ori.append(label_real_from_example)\n",
    "        real_label_ids_ori.append(label_real)\n",
    "    return real_preds_ori, examples_ori, label_real_from_example_ori, real_label_ids_ori\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_entities_bio_mt(seq):\n",
    "    \"\"\"Gets entities from sequence.\n",
    "    note: BIO\n",
    "    Args:\n",
    "        seq (list): sequence of labels.\n",
    "    Returns:\n",
    "        list: list of (chunk_type, chunk_start, chunk_end).\n",
    "    Example:\n",
    "        seq = ['PER', 'PER', 'O', 'LOC', 'PER']\n",
    "        get_entity_bio(seq)\n",
    "        #output\n",
    "        [['PER', 0,1], ['LOC', 3, 3]]\n",
    "    \"\"\"\n",
    "    if any(isinstance(s, list) for s in seq):\n",
    "        seq = [item for sublist in seq for item in sublist + ['O']]\n",
    "    chunks = []\n",
    "    chunk = [-1, -1, -1]\n",
    "    for indx, tag in enumerate(seq):\n",
    "        if tag != 'O':\n",
    "            if chunk[2] != -1 and chunk[0] == tag:\n",
    "                chunk[2] = indx\n",
    "            elif chunk[2] != -1 and chunk[0] != tag:\n",
    "                chunks.append(chunk)\n",
    "                chunk = [-1, -1, -1]\n",
    "                chunk[1] = indx\n",
    "                chunk[0] = tag\n",
    "                chunk[2] = indx\n",
    "            else:\n",
    "                chunk = [-1, -1, -1]\n",
    "                chunk[1] = indx\n",
    "                chunk[0] = tag\n",
    "                chunk[2] = indx\n",
    "            if indx == len(seq) - 1:\n",
    "                chunks.append(chunk)\n",
    "        else:\n",
    "            if chunk[2] != -1:\n",
    "                chunks.append(chunk)\n",
    "            chunk = [-1, -1, -1]\n",
    "    return set([tuple(chunk) for chunk in chunks])\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InputExample_pred_mt:\n",
    "    xml_path: str\n",
    "    words: List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_entities_bio(seq):\n",
    "    \"\"\"Gets entities from sequence.\n",
    "    note: BIOS\n",
    "    Args:\n",
    "        seq (list): sequence of labels.\n",
    "    Returns:\n",
    "        list: list of (chunk_type, chunk_start, chunk_end).\n",
    "    Example:\n",
    "        # >>> seq = ['B-PER', 'I-PER', 'O', 'S-LOC']\n",
    "        # >>> get_entity_bios(seq)\n",
    "        [['PER', 0,1], ['LOC', 3, 3]]\n",
    "    \"\"\"\n",
    "    if any(isinstance(s, list) for s in seq):\n",
    "        seq = [item for sublist in seq for item in sublist + ['O']] # 一定会以o为结尾\n",
    "    chunks = []\n",
    "    chunk = [-1, -1, -1]\n",
    "    for indx, tag in enumerate(seq):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if chunk[2] != -1:\n",
    "                chunks.append(chunk)\n",
    "            chunk = [-1, -1, -1]\n",
    "            chunk[1] = indx\n",
    "            chunk[0] = tag.split('-')[1]\n",
    "            chunk[2] = indx\n",
    "            if indx == len(seq) -1:\n",
    "                chunks.append(chunk)\n",
    "        elif tag.startswith('I-') and chunk[1] != -1:\n",
    "            _type = tag.split('-')[1]\n",
    "            if _type == chunk[0]:\n",
    "                chunk[2] = indx\n",
    "            \n",
    "            if indx == len(seq) - 1:\n",
    "                chunks.append(chunk)\n",
    "        else:\n",
    "            if chunk[2] != -1:\n",
    "                chunks.append(chunk)\n",
    "            chunk = [-1, -1, -1]\n",
    "    return set([tuple(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_bio(list_of_list_seq):\n",
    "    # ['PER', 'PER', 'O', 'LOC', 'PER'] -> ['B-PER', 'I-PER', 'O', 'B-LOC', 'B-PER']\n",
    "    new_list = []\n",
    "    for list_seq in list_of_list_seq:\n",
    "        list_seq_copy = copy.copy(list_seq)\n",
    "        ents = get_entities_bio_mt(list_seq)\n",
    "        for span in ents:\n",
    "            name, start, end = span\n",
    "            list_seq_copy[start] = f\"B-{name}\"\n",
    "            for left_index in range(start+1,end+1):\n",
    "                list_seq_copy[left_index] = f\"I-{name}\"\n",
    "        assert get_entities_bio(list_seq_copy) == get_entities_bio_mt(list_seq)\n",
    "        new_list.append(list_seq_copy)\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsNumberWithOptionalCurrency(s, currency_list):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    if len(s) > 0:\n",
    "        for key in currency_list:\n",
    "            if s[-len(key):] == key:\n",
    "                return IsNumberWithOptionalCurrency(s[:-len(key)], currency_list)\n",
    "\n",
    "    return False\n",
    "\n",
    "def generate_currency_dic(currencyListFilePath):\n",
    "    currencyListD = {}\n",
    "    for line in open(currencyListFilePath, 'r', encoding='utf-8'):\n",
    "        if currencyListD.__contains__(line.strip()):\n",
    "            print(line.strip())\n",
    "        currencyListD[line.strip()] = 1\n",
    "    return currencyListD\n",
    "\n",
    "def compare_number_for_price(location1, location2, currency_list):\n",
    "    number_a = [token for token in location1 if IsNumberWithOptionalCurrency(token, currency_list)]\n",
    "    number_b = [token for token in location2 if IsNumberWithOptionalCurrency(token, currency_list)]\n",
    "    return number_a == number_b\n",
    "\n",
    "\n",
    "def compare(entity, location1, location2, words, currency_list, isprint):\n",
    "    if entity == 'MainImage' and abs(location2[0] - location1[0]) <= 5:\n",
    "        if isprint:\n",
    "            print(\"_iamge_\", entity, words[location1[0]: location1[1] + 1], words[location2[0]: location2[1] + 1])\n",
    "        return True\n",
    "    if words[location1[0]: location1[1] + 1] == words[location2[0]: location2[1] + 1]:\n",
    "        if isprint:\n",
    "            print(\"_1_\", entity, words[location1[0]: location1[1] + 1], words[location2[0]: location2[1] + 1])\n",
    "        return True\n",
    "    if (min(location1[1], location2[1]) - max(location1[0], location2[0]) + 1) / (location2[1] - location2[0] + 1) > 0.6:\n",
    "        if isprint:\n",
    "            print(\"_2_\", entity, words[location1[0]: location1[1] + 1], words[location2[0]: location2[1] + 1])\n",
    "        return True\n",
    "    Longest = longestCommonSubstring(' '.join(words[location1[0]: location1[1] + 1]), ' '.join(words[location2[0]: location2[1] + 1]))\n",
    "    if Longest / len(' '.join(words[location2[0]: location2[1] + 1])) > 0.6 or (entity == 'Price' and len(words[location1[0]: location1[1] + 1]) == 2):\n",
    "        if entity == 'Price' and compare_number_for_price(words[location1[0]: location1[1] + 1], words[location2[0]: location2[1] + 1], currency_list) is False:\n",
    "            if isprint:\n",
    "                print(\"_4_\", entity, words[location1[0]: location1[1] + 1], words[location2[0]: location2[1] + 1])\n",
    "            return False\n",
    "        if isprint:\n",
    "            print(\"_3_\", entity, words[location1[0]: location1[1] + 1], words[location2[0]: location2[1] + 1])\n",
    "        return True\n",
    "    if isprint:\n",
    "        print(\"_4_\", entity, words[location1[0]: location1[1] + 1], words[location2[0]: location2[1] + 1])\n",
    "    return False\n",
    "\n",
    "def get_metric(real_preds, real_label_ids, examples, currency_list, isprint=False):\n",
    "    real_results, real_label_results = {}, {}\n",
    "    res_pred, res_label = [], []\n",
    "    for index_i, real_pred in enumerate(real_preds):\n",
    "        for index_j, label in enumerate(real_pred):\n",
    "            if 'O' == label:\n",
    "                continue\n",
    "            key_v = label.split('-')\n",
    "            if real_results.__contains__(key_v[1]) is False:\n",
    "                real_results[key_v[1]] = []\n",
    "            if 'B' == key_v[0]:\n",
    "                real_results[key_v[1]].append([index_j, index_j])\n",
    "            if 'I' == key_v[0]:\n",
    "                if len(real_results[key_v[1]]) == 0:\n",
    "                    # real_results[key_v[1]].append([index_j, index_j])\n",
    "                    continue\n",
    "                else:\n",
    "                    real_results[key_v[1]][-1][-1] += 1\n",
    "        res_pred.append(real_results)\n",
    "        real_results = {}\n",
    "\n",
    "        for index_j, label in enumerate(real_label_ids[index_i]):\n",
    "            if 'O' == label:\n",
    "                continue\n",
    "            key_v = label.split('-')\n",
    "            if real_label_results.__contains__(key_v[1]) is False:\n",
    "                real_label_results[key_v[1]] = []\n",
    "            if 'B' == key_v[0]:\n",
    "                real_label_results[key_v[1]].append([index_j, index_j])\n",
    "            if 'I' == key_v[0]:\n",
    "                if not real_label_results[key_v[1]]:\n",
    "                    real_label_results[key_v[1]].append([index_j, index_j])\n",
    "                else:\n",
    "                    real_label_results[key_v[1]][-1][-1] = index_j\n",
    "        res_label.append(real_label_results)\n",
    "        real_label_results = {}\n",
    "    for index, pred in enumerate(res_pred):\n",
    "        for key in pred:\n",
    "            if res_label[index].__contains__(key) is False:\n",
    "                continue\n",
    "            labels = res_label[index][key][0]\n",
    "            for number in range(len(pred[key])):\n",
    "                if pred[key][number] != labels and compare(key, pred[key][number], labels, examples[index].words, currency_list, isprint):\n",
    "                    for offset in range(pred[key][number][0], pred[key][number][1] + 1):\n",
    "                        real_preds[index][offset] = 'O'\n",
    "                    for offset in range(labels[0], labels[1] + 1):\n",
    "                        if offset == labels[0]:\n",
    "                            real_preds[index][offset] = 'B-' + key\n",
    "                        else:\n",
    "                            real_preds[index][offset] = 'I-' + key\n",
    "                    pred[key][number] = labels\n",
    "\n",
    "                    \n",
    "\n",
    "    return compute_metric_mt_add_qiang(real_preds, real_label_ids)\n",
    "\n",
    "def f1_score_ee(true_entities, pred_entities):\n",
    "    \"\"\"Compute the F1 score for DeepEE.\"\"\"\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "    r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "    score = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def precision_score_ee(true_entities, pred_entities):\n",
    "    \"\"\"Compute the precision for DeepEE.\"\"\"\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "\n",
    "    score = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def recall_score_ee(true_entities, pred_entities):\n",
    "    \"\"\"Compute the recall for DeepEE.\"\"\"\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    score = nb_correct / nb_true if nb_true > 0 else 0\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def classification_report_mt(true_entities, pred_entities, digits=5):\n",
    "    \"\"\"Build a text report showing the main classification metrics.\"\"\"\n",
    "    name_width = 0\n",
    "    d1 = defaultdict(set)\n",
    "    d2 = defaultdict(set)\n",
    "    for e in true_entities:\n",
    "        d1[e[0]].add((e[1], e[2]))\n",
    "        name_width = max(name_width, len(e[0]))\n",
    "    for e in pred_entities:\n",
    "        d2[e[0]].add((e[1], e[2]))\n",
    "\n",
    "\n",
    "    last_line_heading = 'macro avg'\n",
    "    width = max(name_width, len(last_line_heading), digits)\n",
    "\n",
    "    headers = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)\n",
    "    report = head_fmt.format(u'', *headers, width=width)\n",
    "    report += u'\\n\\n'\n",
    "\n",
    "    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\\n'\n",
    "\n",
    "    ps, rs, f1s, s = [], [], [], []\n",
    "    for type_name, type_true_entities in d1.items():\n",
    "        type_pred_entities = d2[type_name]\n",
    "        nb_correct = len(type_true_entities & type_pred_entities)\n",
    "        nb_pred = len(type_pred_entities)\n",
    "        nb_true = len(type_true_entities)\n",
    "\n",
    "        p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "        r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "        report += row_fmt.format(*[type_name, p, r, f1, nb_true], width=width, digits=digits)\n",
    "\n",
    "        ps.append(p)\n",
    "        rs.append(r)\n",
    "        f1s.append(f1)\n",
    "        s.append(nb_true)\n",
    "\n",
    "    report += u'\\n'\n",
    "\n",
    " \n",
    "\n",
    "    # compute averages\n",
    "    report += row_fmt.format('micro avg',\n",
    "                             precision_score_ee(true_entities, pred_entities),\n",
    "                             recall_score_ee(true_entities, pred_entities),\n",
    "                             f1_score_ee(true_entities, pred_entities),\n",
    "                             np.sum(s),\n",
    "                             width=width, digits=digits)\n",
    "    report += row_fmt.format(last_line_heading,\n",
    "                             np.average(ps, weights=s),\n",
    "                             np.average(rs, weights=s),\n",
    "                             np.average(f1s, weights=s),\n",
    "                             np.sum(s),\n",
    "                             width=width, digits=digits)\n",
    "\n",
    " \n",
    "\n",
    "    return report\n",
    "\n",
    "def compute_metric_mt_add_qiang(prediction_onnx_origin, real_label_ids_ori):\n",
    "    true_entities = get_entities_bio(real_label_ids_ori)\n",
    "    pred_entities = get_entities_bio(prediction_onnx_origin)\n",
    "\n",
    "    bad_pred = [ett for ett in pred_entities if ett not in true_entities]\n",
    "    print(bad_pred)\n",
    "\n",
    "    print('-'*10)\n",
    "    print(true_entities)\n",
    "\n",
    "    \n",
    "    results = {\n",
    "        \"f1\": f1_score_ee(true_entities, pred_entities),\n",
    "        'report': classification_report_mt(true_entities, pred_entities),\n",
    "        'recall':recall_score_ee(true_entities, pred_entities),\n",
    "        'precision':precision_score_ee(true_entities, pred_entities)\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_list = np.load('./data/currency.npy', allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_different_model(saved_dir, all_candidate):\n",
    "    all_candidate = [all_candidate]\n",
    "    all_cand_res = {}\n",
    "    for one_model_candidate in all_candidate:\n",
    "        print(f\"[Info] start for candidate {one_model_candidate}\")\n",
    "        args.model_name_or_path = one_model_candidate\n",
    "        model_path_or_model = args.model_name_or_path\n",
    "        config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "        model_config = config_class.from_pretrained(args.model_name_or_path,\n",
    "                                                  finetuning_task=\"dummy\",\n",
    "                                                  cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "        add_config(model_config)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "                YOUR_MODEL_PATH,\n",
    "                use_fast=True,\n",
    "            )\n",
    "        model_path_or_model = model_class.from_pretrained(\n",
    "                    model_path_or_model,\n",
    "                    from_tf=False,\n",
    "                    config=model_config,\n",
    "                    cache_dir=None,\n",
    "                )\n",
    "        \n",
    "        saved_lang_result_for_this_candidate = {}\n",
    "        \n",
    "        for one_conll_file_path in all_conll_file_path:\n",
    "            print(\"*\"*10)\n",
    "            args.conll_file_path = one_conll_file_path\n",
    "            language_name = one_conll_file_path.split('/')[-2]\n",
    "            entity_type = os.path.basename(one_conll_file_path).split('_')[0]\n",
    "            print(f\"[Info]: Metrics for language {language_name} entity type {entity_type}\")\n",
    "            label_this = ['O', entity_type]\n",
    "            label2id_this = {\"O\":0, entity_type:1}\n",
    "            label_map_this = {0:\"O\", 1:entity_type}\n",
    "            overlap_conll = trans_conll_file_to_overlap_file(args.conll_file_path, True)\n",
    "            test_dataset = LabelingDataset_mt(overlap_conll, tokenizer, label_this, args.vocab_clip_mapping_file is not None, vocab_clip_mapping, 512, model_config)\n",
    "            test_dataloader = get_eval_dataloader(test_dataset, batch_size=model_config.batch_size)\n",
    "            task_index_map = {\"MainImage\":0, \"Price\":2, \"Name\":1}\n",
    "\n",
    "            \n",
    "            model_config.task_index = task_index_map[entity_type]\n",
    "\n",
    "            \n",
    "            test_label_ids, prediction_onnx = inference_mt(test_dataloader, model_path_or_model, task_index_map[entity_type], model_config)\n",
    "            real_prediction_onnx, real_label_ids = get_real_label_mt(prediction_onnx, test_label_ids, label_map_this, label2id_this)\n",
    "            real_prediction_onnx_origin, examples_origin, label_real_from_example_ori, real_label_ids_ori= get_real_result_from_real_preds_add_mt(real_prediction_onnx, test_dataset.__get_examples__(), real_label_ids)\n",
    "            \n",
    "            real_prediction_onnx_origin_optimized = change_to_bio(real_prediction_onnx_origin)\n",
    "\n",
    "            real_label_ids_ori_optimized = change_to_bio(real_label_ids_ori)\n",
    "\n",
    "            out_res = get_metric(real_prediction_onnx_origin_optimized, real_label_ids_ori_optimized, examples_origin, currency_list)\n",
    "\n",
    "\n",
    "            print(out_res)\n",
    "            print('*'*10)\n",
    "            if language_name in saved_lang_result_for_this_candidate:\n",
    "                saved_lang_result_for_this_candidate[language_name][entity_type]['recall'] = out_res['recall']\n",
    "                saved_lang_result_for_this_candidate[language_name][entity_type]['precision'] = out_res['precision']\n",
    "            else:\n",
    "                saved_lang_result_for_this_candidate[language_name] = {\"Price\":{}, \"MainImage\":{}, \"Name\":{}}\n",
    "                saved_lang_result_for_this_candidate[language_name][entity_type]['recall'] = out_res['recall']\n",
    "                saved_lang_result_for_this_candidate[language_name][entity_type]['precision'] = out_res['precision']\n",
    "\n",
    "        all_cand_res[one_model_candidate] = saved_lang_result_for_this_candidate\n",
    "    return all_cand_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "all_conll_file_full_paths = []\n",
    "for ll in os.listdir(\"./test_data\"):\n",
    "    this_ll = os.path.join(\"./test_data\", ll)\n",
    "    xixi = glob(os.path.join(this_ll, \"*_conll.tsv\"))\n",
    "    all_conll_file_full_paths.extend(xixi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic(res_dict):\n",
    "    all_ckp = res_dict.keys()\n",
    "    for one_ckp in all_ckp:\n",
    "        this_ckp_result = res_dict[one_ckp]\n",
    "        block_1, block_2, block_3, block_4, all_precision, all_recall = [], [], [], [], [], []\n",
    "        all_langs = this_ckp_result.keys()\n",
    "        for one_langs in all_langs:\n",
    "            this_lang_metrics = this_ckp_result[one_langs]\n",
    "\n",
    "            try:\n",
    "                price_precison = this_lang_metrics['Price']['precision']\n",
    "            except:\n",
    "                price_precison = 100\n",
    "            \n",
    "            try:\n",
    "                price_recall = this_lang_metrics['Price']['recall']\n",
    "            except:\n",
    "                price_recall = 100\n",
    "\n",
    "            try:\n",
    "                image_precision = this_lang_metrics['MainImage']['precision']\n",
    "            except:\n",
    "                image_precision = 100\n",
    "\n",
    "            try:\n",
    "                image_recall = this_lang_metrics['MainImage']['recall']\n",
    "            except:\n",
    "                image_recall = 100\n",
    "\n",
    "            try:\n",
    "                name_precision = this_lang_metrics['Name']['precision']\n",
    "            except:\n",
    "                name_precision = 100\n",
    "\n",
    "            try:\n",
    "                name_recall = this_lang_metrics['Name']['recall']\n",
    "            except:\n",
    "                name_recall = 100\n",
    "\n",
    "            \n",
    "            all_values = [price_precison, price_recall, image_precision, image_recall, name_precision, name_recall]\n",
    "            min_value = min(all_values)\n",
    "            if min_value >= 0.9:\n",
    "                block_4.append(one_langs)\n",
    "            elif min_value >= 0.8:\n",
    "                block_3.append(one_langs)\n",
    "            elif min_value >= 0.7:\n",
    "                block_2.append(one_langs)\n",
    "            else:\n",
    "                block_1.append(one_langs)\n",
    "            all_precision.extend([price_precison, image_precision, name_precision])\n",
    "            all_recall.extend([price_recall, image_recall, name_recall])\n",
    "            all_precision = [i for i in all_precision if i!= 100]\n",
    "            all_recall = [i for i in all_recall if i!= 100]\n",
    "     \n",
    "        print(f\"For checkpoint {one_ckp}, <70 markets number: {len(block_1)}, 70-80: {len(block_2)}, 80-90: {len(block_3)}, >90: {len(block_4)}, avg micro precision: {np.mean(all_precision)}, avg micro recall: {np.mean(all_recall)}, avg micro f1: {2 * np.mean(all_precision) * np.mean(all_recall) / (np.mean(all_precision) + np.mean(all_recall))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the final resuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_clip_mapping = {}\n",
    "res = run_different_model(\"./saved/test\", all_conll_file_full_paths)\n",
    "statistic(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
